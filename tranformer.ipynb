{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cc274c-3d7f-44a3-8a6a-95d74e2b99f7",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e477fd0-12e8-48ac-938e-e15e92c04f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'zarr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzarr\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'zarr'"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "store = zarr.open('processed_data_cse151b_v2_corrupted_ssp245.zarr', mode='r')\n",
    "store.tree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25be1c8-e5e6-4289-8094-82baebf5c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    test = data[1]\n",
    "    \n",
    "    temp = np.concatenate([data[0], data[3], data[2]])\n",
    "    train = temp[:-120]\n",
    "    val = temp[-120:]\n",
    "    return test, train, val\n",
    "\n",
    "\n",
    "f1 = store[\"BC\"][:].reshape(4, 1021, 3456)\n",
    "f2 = store[\"SO2\"][:].reshape(4, 1021, 3456)\n",
    "f3 = store[\"rsdt\"][:].reshape(4, 1021, 3456)\n",
    "\n",
    "\n",
    "f4 = store[\"CO2\"][:]\n",
    "f5 = store[\"CH4\"][:] \n",
    "f4 = np.repeat(f4[:, :, np.newaxis], 3456, axis=2)\n",
    "f5 = np.repeat(f5[:, :, np.newaxis], 3456, axis=2)\n",
    "\n",
    "f6 = store[\"time\"][:] \n",
    "f6 = np.array([f6, f6, f6, f6])\n",
    "f6 = np.repeat(f6[:, :, np.newaxis], 3456, axis=2)\n",
    "\n",
    "t1 = store[\"pr\"][:].mean(axis=2).reshape(4, 1021, 3456)\n",
    "t2 = store[\"tas\"][:].mean(axis=2).reshape(4, 1021, 3456)\n",
    "\n",
    "data = np.array([f1.T, f2.T, f3.T, f4.T, f5.T, f6.T, t1.T, t2.T]).T\n",
    "\n",
    "X = np.array([f1.T, f2.T, f3.T, f4.T, f5.T, f6.T]).T\n",
    "y = t2\n",
    "\n",
    "X_test, X_train, X_val = train_test_split(X)\n",
    "y_test, y_train, y_val = train_test_split(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68523f9f-a7e3-406e-83a9-21451b95f134",
   "metadata": {},
   "source": [
    "# Tranformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e180ce59-f3c0-47cf-88c7-ac53f98a7303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train: 100%|██████████| 1472/1472 [03:04<00:00,  7.99it/s]\n",
      "Epoch 1 - Val: 100%|██████████| 60/60 [00:00<00:00, 306.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train RMSE: 110.82 | Val RMSE: 20.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|██████████| 1472/1472 [03:04<00:00,  7.97it/s]\n",
      "Epoch 2 - Val: 100%|██████████| 60/60 [00:00<00:00, 308.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train RMSE: 35.16 | Val RMSE: 21.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|██████████| 1472/1472 [03:04<00:00,  7.97it/s]\n",
      "Epoch 3 - Val: 100%|██████████| 60/60 [00:00<00:00, 307.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train RMSE: 26.57 | Val RMSE: 22.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|██████████| 1472/1472 [03:04<00:00,  7.96it/s]\n",
      "Epoch 4 - Val: 100%|██████████| 60/60 [00:00<00:00, 305.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train RMSE: 23.58 | Val RMSE: 21.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|██████████| 1472/1472 [03:05<00:00,  7.94it/s]\n",
      "Epoch 5 - Val: 100%|██████████| 60/60 [00:00<00:00, 295.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train RMSE: 21.95 | Val RMSE: 21.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|██████████| 1472/1472 [03:04<00:00,  7.97it/s]\n",
      "Epoch 6 - Val: 100%|██████████| 60/60 [00:00<00:00, 307.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train RMSE: 20.37 | Val RMSE: 19.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train: 100%|██████████| 1472/1472 [03:04<00:00,  7.97it/s]\n",
      "Epoch 7 - Val: 100%|██████████| 60/60 [00:00<00:00, 304.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train RMSE: 18.64 | Val RMSE: 17.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train: 100%|██████████| 1472/1472 [03:04<00:00,  7.97it/s]\n",
      "Epoch 8 - Val: 100%|██████████| 60/60 [00:00<00:00, 301.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train RMSE: 17.54 | Val RMSE: 18.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train:   6%|▌         | 83/1472 [00:10<02:56,  7.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     90\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 91\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     94\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# === Positional encoding (add to input) ===\n",
    "H, W = 48, 72\n",
    "grid_y, grid_x = np.meshgrid(np.linspace(0, 1, H), np.linspace(0, 1, W), indexing='ij')\n",
    "pos = np.stack([grid_y, grid_x], axis=-1).reshape(3456, 2)\n",
    "\n",
    "# Add 2D position to input (X_train and X_val must be defined already)\n",
    "X_train = np.concatenate([X_train, np.broadcast_to(pos, (X_train.shape[0], 3456, 2))], axis=-1)\n",
    "X_val = np.concatenate([X_val, np.broadcast_to(pos, (X_val.shape[0], 3456, 2))], axis=-1)\n",
    "\n",
    "# === Target scaling ===\n",
    "SCALE_FACTOR = 300.0\n",
    "y_train_scaled = y_train / SCALE_FACTOR\n",
    "y_val_scaled = y_val / SCALE_FACTOR\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# === Model ===\n",
    "class PositionWiseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=8, model_dim=64, num_heads=2, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv1d(input_dim, model_dim, kernel_size=5, padding=2)\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=128,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(model_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)         # (B, 8, 3456)\n",
    "        x = self.cnn(x)               # (B, model_dim, 3456)\n",
    "        x = x.transpose(1, 2)         # (B, 3456, model_dim)\n",
    "        x = self.norm(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.output_proj(x)\n",
    "        return x.squeeze(-1)          # (B, 3456)\n",
    "\n",
    "# === Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PositionWiseTransformer().to(device)\n",
    "\n",
    "# Xavier init\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# LR Warmup scheduler\n",
    "def lr_lambda(step):\n",
    "    warmup_steps = 200\n",
    "    return min((step + 1) / warmup_steps, 1.0)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# === Training ===\n",
    "step = 0\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        step += 1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_preds = model(xb)\n",
    "            val_loss += criterion(val_preds, yb).item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train RMSE: {SCALE_FACTOR * avg_train_loss**0.5:.2f} | Val RMSE: {SCALE_FACTOR * avg_val_loss**0.5:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138e20f-cee3-4ba3-9d1d-e2dc442dea81",
   "metadata": {},
   "source": [
    "# Vision Tranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e673ad6-654e-47aa-adc6-1dcaae7e7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Parameters ===\n",
    "H, W = 48, 72\n",
    "PATCH_H, PATCH_W = 6, 6\n",
    "NUM_PATCHES = (H // PATCH_H) * (W // PATCH_W)\n",
    "PATCH_SIZE = PATCH_H * PATCH_W\n",
    "IN_CHANNELS = 8\n",
    "PATCH_DIM = PATCH_SIZE * IN_CHANNELS\n",
    "EMBED_DIM = 128\n",
    "OUT_DIM = 2  # pr and tas\n",
    "\n",
    "# === Dataset setup (same as before, with positional encoding) ===\n",
    "grid_y, grid_x = np.meshgrid(np.linspace(0, 1, H), np.linspace(0, 1, W), indexing='ij')\n",
    "pos = np.stack([grid_y, grid_x], axis=-1).reshape(H * W, 2)\n",
    "X_train = np.concatenate([X_train, np.broadcast_to(pos, (X_train.shape[0], H * W, 2))], axis=-1)\n",
    "X_val = np.concatenate([X_val, np.broadcast_to(pos, (X_val.shape[0], H * W, 2))], axis=-1)\n",
    "\n",
    "SCALE_FACTOR = 300.0\n",
    "y_train_scaled = y_train / SCALE_FACTOR\n",
    "y_val_scaled = y_val / SCALE_FACTOR\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).reshape(-1, H, W, IN_CHANNELS)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).reshape(-1, H, W, IN_CHANNELS)\n",
    "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# === ViT-style model ===\n",
    "class ViTRegression(nn.Module):\n",
    "    def __init__(self, in_channels=8, patch_size=6, embed_dim=128, out_dim=2, image_size=(48, 72)):\n",
    "        super().__init__()\n",
    "        self.patch_h, self.patch_w = patch_size, patch_size\n",
    "        self.H, self.W = image_size\n",
    "        self.num_patches = (self.H // self.patch_h) * (self.W // self.patch_w)\n",
    "        patch_dim = in_channels * patch_size * patch_size\n",
    "\n",
    "        self.proj = nn.Linear(patch_dim, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(self.num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, out_dim)  # output: pr and tas\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        assert H == self.H and W == self.W\n",
    "\n",
    "        # Divide into patches\n",
    "        x = x.unfold(1, self.patch_h, self.patch_h).unfold(2, self.patch_w, self.patch_w)\n",
    "        x = x.contiguous().view(B, -1, C, self.patch_h, self.patch_w)  # (B, N_patches, C, 6, 6)\n",
    "        x = x.flatten(3).flatten(2)  # (B, N_patches, C*6*6)\n",
    "\n",
    "        x = self.proj(x) + self.pos_embed  # (B, N_patches, D)\n",
    "        x = self.transformer(x)\n",
    "        out = self.regressor(x)  # (B, N_patches, 2)\n",
    "        return out.view(B, -1, 2)\n",
    "\n",
    "# === Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViTRegression().to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    warmup_steps = 200\n",
    "    return min((step + 1) / warmup_steps, 1.0)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# === Training loop ===\n",
    "step = 0\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"):\n",
    "        xb, yb = xb.to(device), yb.to(device).view(xb.shape[0], -1, 2)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        step += 1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
    "            xb, yb = xb.to(device), yb.to(device).view(xb.shape[0], -1, 2)\n",
    "            val_preds = model(xb)\n",
    "            val_loss += criterion(val_preds, yb).item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    rmse_train = SCALE_FACTOR * avg_train_loss**0.5\n",
    "    rmse_val = SCALE_FACTOR * avg_val_loss**0.5\n",
    "    print(f\"Epoch {epoch+1} | Train RMSE: {rmse_train:.2f} | Val RMSE: {rmse_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d07e1-a8d9-48cf-8d0a-176818641e7c",
   "metadata": {},
   "source": [
    "# Result Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669ebd8e-a425-491d-bc76-5affbd00fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x_sample = X_val_tensor[0].unsqueeze(0).to(device)  # (1, 3456, 6)\n",
    "y_true = y_val_tensor[0].cpu().numpy().reshape(48, 72)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_sample).cpu().numpy().squeeze().reshape(48, 72)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(y_true, ax=axes[0], cmap=\"viridis\")\n",
    "axes[0].set_title(\"Ground Truth\")\n",
    "\n",
    "sns.heatmap(y_pred, ax=axes[1], cmap=\"viridis\")\n",
    "axes[1].set_title(\"Model Prediction\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32833d9f-e931-4c8f-be08-fef2bc186cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cse151)",
   "language": "python",
   "name": "cse151"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
